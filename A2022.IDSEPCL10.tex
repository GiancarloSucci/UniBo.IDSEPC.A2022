\documentclass{beamer}
%
% Choose how your presentation looks.
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{Madrid}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{seahorse} % or try albatross, beaver, crane, ...
  \usefonttheme{serif}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \usepackage{amsmath}
  \usepackage{tcolorbox}
  \usepackage[export]{adjustbox}
  \tcbuselibrary{most}
  \usepackage{amsmath}  % conflicts with \oiint
  \usepackage{arydshln}
  \usepackage{tikz}
  \usetikzlibrary{plotmarks}
  \usepackage{pgfplots}
  \newcommand{\Ivec}[1]{\mbox{\boldmath $#1$}}
    \usepackage[normalem]{ulem}
    % \usepackage{tipa}
    \newcommand{\git}{\texttt{\textbf{git}}\xspace}
    \usepackage{listings}
} 


\definecolor{myblue}{RGB}{65,105,225} 
\definecolor{myorange}{RGB}{250,190,0}

\setbeamercolor{structure}{fg=white,bg=myorange}
\setbeamercolor*{palette primary}{fg=myblue,bg=myorange}
\setbeamercolor*{palette secondary}{fg=white,bg=myblue}
\setbeamercolor*{palette tertiary}{bg=myblue,fg=white}
\setbeamercolor*{palette quaternary}{fg=white,bg=myorange!50}

\setbeamercolor{frametitle}{fg=black!90!myblue}

\setbeamercolor{section in head/foot}{fg=white,bg=myblue}
\setbeamercolor{author in head/foot}{fg=black,bg=myorange}
\setbeamercolor{title in head/foot}{fg=white,bg=myblue}

\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{itemize/enumerate body begin}{\large}
\setbeamertemplate{itemize/enumerate subbody begin}{\large}


\defbeamertemplate*{headline}{mytheme}
{%
  \begin{beamercolorbox}[ht=2.25ex,dp=3.75ex]{section in head/foot}
    \insertnavigation{\paperwidth}
  \end{beamercolorbox}%
}%

\defbeamertemplate*{footline}{mytheme}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,right]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor\hspace*{2em}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,left]{title in head/foot}%
    \usebeamerfont{title in head/foot}\hspace*{2em}\insertshortsubtitle\hspace*{2em}
    \insertframenumber{} / \inserttotalframenumber
  \end{beamercolorbox}}%
  \vskip0pt%
}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{pgf}  
\usepackage{textpos}
\usepackage{tabulary}
\usepackage{scrextend}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{rotating}
\lstset
{
    language=[LaTeX]TeX,
    breaklines=true,
    basicstyle=\tt\scriptsize,
    %commentstyle=\color{green}
    keywordstyle=\color{blue},
    %stringstyle=\color{black}
    identifierstyle=\color{magenta},
}
\newcommand{\bftt}[1]{\textbf{\texttt{#1}}}
%\newcommand{\comment}[1]{{\color[HTML]{008080}\textit{\textbf{\texttt{#1}}}}}
\newcommand{\cmd}[1]{{\color[HTML]{008000}\bftt{#1}}}
\newcommand{\bs}{\char`\\}
\newcommand{\cmdbs}[1]{\cmd{\bs#1}}
\newcommand{\lcb}{\char '173}
\newcommand{\rcb}{\char '175}
\newcommand{\cmdbegin}[1]{\cmdbs{begin\lcb}\bftt{#1}\cmd{\rcb}}
\newcommand{\cmdend}[1]{\cmdbs{end\lcb}\bftt{#1}\cmd{\rcb}}

\newcommand{\wllogo}{\textbf{Overleaf}}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

% this is where the example source files are loaded from
% do not include a trailing slash
\newcommand{\fileuri}{https://raw.githubusercontent.com/GiancarloSucci/UniBo.IDSEPC.A2022/main/A2022.IDSEPCLaTeX/}

\include{definizioniGiancarlo.tex}

\usepackage{stackengine}
\def\Ruble{\stackengine{.67ex}{%
  \stackengine{.48ex}{\textsf{P}}{\rule{.8ex}{.12ex}\kern.6ex}{O}{r}{F}{F}{L}%
  }{\rule{.8ex}{.12ex}\kern.6ex}{O}{r}{F}{F}{L}\kern-.1ex}



%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------
\title[L02]{Introduzione alla data science e al pensiero computazionale\\
Lezione 10: Correlazione} % The short title appears at the bottom of every slide, the full title is only on the title page

\author[{\tiny Giancarlo Succi }]{Giancarlo Succi\\\\ Dipartimento di Informatica -- Scienza e Ingegneria\\Universit\`{a} di Bologna\\
\bftt{g.succi@unibo.it}
} % Your name
\institute[unibo] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space

\date{} % Date, can be changed to a custom date

\setbeamertemplate{navigation symbols}{}
\AtBeginSection[]
{
        \begin{frame}<beamer>{Outline}
                \tableofcontents[currentsection]
        \end{frame}
}

\begin{document}
\begin{frame}
\titlepage % Print the title page as the first slide

\end{frame}

\begin{frame}
{\centerline{Content}}

\begin{itemize}
\item Covariance
\item Correlation (aka Pearson product-moment correlation coefficient)
\item Relationship between Pearson correlation and linear regression
\item Non parametric correlations
\end{itemize}
\end{frame}


\begin{frame}
{\centerline{Covariance}}

\begin{itemize}
\item To proceed further with our analysis we will use the concept of \textbf{covariance}, which we have already seen
\item It expresses the degree in which the variation of a random variable is connected to the variation of another random variable
\item It is defined as follows:
\begin{itemize}
\item Given two random variables $X$ and $Y$
$$Cov(X,Y) = E[ (X - E(X))(Y - E(Y))]$$
\end{itemize}
\end{itemize}

\end{frame}


\begin{frame}
{\centerline{Covariance -- graphically}}


\begin{center}
\includegraphics[width=10cm]{A2022.IDSEPC.Correlazione/Covar.png}
\end{center} 
\textit{\small
Source : \url{https://www.geeksforgeeks.org/mathematics-covariance-and-correlation}}
\end{frame}


\begin{frame}
{\centerline{About the covariance - 1}}

\begin{itemize}
\item We notice that:
\begin{itemize}
\item The covariance of a random variable with itself is the variance:
$$Cov(X,X) = Var(X)$$
\item There is a similar property as for the variance
$$Cov(X,Y) = E(XY) - E(X)E(Y)$$
since:
$$Cov(X,Y) = E[ (X - E(X))(Y - E(Y))] = $$
$$ = E(XY) - E(XE(Y)) - E(E(X)Y) + E(E(X)E(Y)) =$$
$$ = E(XY) - E(X)E(Y) - E(X)E(Y) + E(X)E(Y) = $$
$$ = E(XY) - E(X)E(Y)$$
QED.


\end{itemize}


\end{itemize}

\end{frame}


\begin{frame}
{\centerline{About the covariance - 2}} \label{P:Covariance2}

\begin{itemize}
\item The covariance is symmetric:
 $$Cov(X,Y) = Cov(Y,X)$$
\item The covariance is linear with respect to multiplications by constants:
 $$(\forall a, b \in \mathbb{R}) ~~ Cov(aX,bY) = abCov(X,Y)$$

\item If $e \sim N(0,\sigma)$, $Cov(X,e) = 0$ \\
$$Cov(X,e) =  E(Xe) - E(X)E(e)$$
Since  $X$ and $e$ are independent
$$ E(Xe) = E(X)E(e)$$
Moreover,  $e \sim N(0,\sigma)$
$$E(e) = 0$$
QED.
\end{itemize}


\end{frame}


\begin{frame}
{\centerline{Pearson Correlation Coefficient}} \label{P:Pearson}

\begin{itemize}
\item AKA Pearson product-moment correlation coefficient or just correlation coefficient
\item It expresses the linear correlation between two random variables
\item It is defined as follows:
\begin{itemize}
\item Given two random variables $X$ and $Y$
$$r_{X,Y} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$
\item Where:
$$\sigma_Z = \sqrt{Var(Z)}$$
\end{itemize}
\end{itemize}

\vspace*{0.5cm}

\textit{\small For the time being we intentionally ignore the difference between sample and population.}

\end{frame}

\begin{frame}
{\centerline{Pearson Correlation Coefficient -- graphically}}


\begin{center}
\includegraphics[width=10cm]{A2022.IDSEPC.Correlazione/Correl.png}
\end{center} 
\textit{\small
Source : \url{https://www.geeksforgeeks.org/mathematics-covariance-and-correlation}}
\end{frame}


\begin{frame}
{\centerline{About the Pearson Correlation Coefficient}}

\begin{itemize}
\item The Pearson correlation coefficient is also often expressed as:

$$ r_{X,Y} = \frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\overline{x})^2\sum_{i=1}^{n}(y_i-\overline{y})^2}} $$
\item It is symmetric: $r_{X,Y} = r_{Y,X}$
\item It is invariant with respect to multiplications by, and additions of constants: $(\forall a, b, c, d \in \mathbb{R} , b \neq 0, d \neq 0)~~ r_{X,Y} = r_{(a + bX),(c + dY)} $
\item It ranges from -1 to 1: $-1 \leq r_{X,Y} \leq 1$
$ r_{X,Y}=1$ means perfect linear relationship (all points lie on a monotonically increasing line) \\
$ r_{X,Y}=-1$ means perfect opposite linear relationship (all points lie on a monotonically decreasing line)\\
$ r_{X,Y}= 0$ means no linear relationship between $X$ and $Y$
\end{itemize}


\end{frame}

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{Back to Linear Regression (1/2)}}
\begin{itemize}
\item We now focus our attention to the case of the case of the linear regression
\item Suppose we have two phenomena that we want to measure,  $X$ and $Y$
\item Let us assume 
\begin{itemize}
\item that there is a linear relationship between them
\item that I can express the data I collect as:
$$ \boldsymbol y = \boldsymbol \theta_0 + X  \boldsymbol \theta_1 + \boldsymbol \epsilon $$
\item where $\epsilon$ is a stationary gaussian process $N(0,\sigma^2)$
\end{itemize}
\item We know the solution that minimizes the square error
\end{itemize}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{Back to Linear Regression (2/2)}}
\begin{itemize}
\item From this solution we have extracted the coefficient of determination
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$
\item Where: 
\begin{itemize}
\item $SS_{res} = \sum_i (y_i - \hat{y}_i)^2$ 
\item \textcolor{blue}{$SS_{res} $ is the distance between the reality and the 1-degree best approximation, that is, the OLS model}
\end{itemize}
\item and
\begin{itemize}
\item $SS_{tot} = \sum_i (y_i - \overline{y})^2$
\item  \textcolor{blue}{$SS_{tot}$ is the distance between the reality and the 0-degree best approximation, that is, the mean}
\end{itemize}

\item I want to know the relationship between $R^2$ and the correlation coefficient between $X$ and $Y$, $r_{X,Y}$
\end{itemize}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{Our goal -- understanding R and r}}
\begin{itemize}
\item We focus on 1D
\item We are now going to prove a fundamental point.
\item Under the assumption that the noise is gaussian and centered in 0, in a linear regression:
\end{itemize}
$$R^2 = r_{X,Y}^2$$
\end{frame}
% ============= =========== % ============= ===========


% ============= =========== % ============= ===========
\begin{frame}
{\centerline{$R^2 = r_{X,Y}^2$ (1/4)}}
\begin{itemize}
\item Since
$$ \hat{y} = \theta_0 + \theta_1 x$$
\item we have from above (see page \ref{P:Pearson}) that:
$$r_{X,Y} = r_{\hat{Y},Y}$$
\item We define now the explained sum of squares (ESS) 
\begin{itemize}
\item $ ESS = \sum_i (\hat{y}_i - \overline{y})^2$
\item \textcolor{blue}{$ ESS $ is the additional knowledge we get on the random variable using a polynomial of degree 1 vs. using a polynomial of degree 0}
\end{itemize}

\item We will now prove that \textbf{under our hypotheses}:
$$ESS + SS_{res} = SS_{tot} $$
\end{itemize}
\end{frame}
% ============= =========== % ============= ===========


% ============= =========== % ============= ===========
\begin{frame}
{\centerline{ [$R^2 = r_{X,Y}^2$] -- $ESS + SS_{res} = SS_{tot} $ (1/6)}}

\begin{itemize}
\item We start from:
$$(y_{i}-{\overline {y}})=(y_{i}-{\hat {y}}_{i})+({\hat {y}}_{i}-{\overline {y}})$$

\item which we square:
$$(y_{i}-{\overline {y}})^2=(y_{i}-{\hat {y}}_{i})^2 + 2(y_{i}-{\hat {y}}_{i})({\hat {y}}_{i}-{\overline {y}}) + ({\hat {y}}_{i}-{\overline {y}})^2$$

\item and then we sum:
$$\sum_i(y_{i}-{\overline {y}})^2=\sum_i(y_{i}-{\hat {y}}_{i})^2 + \sum_i2(y_{i}-{\hat {y}}_{i})({\hat {y}}_{i}-{\overline {y}}) + \sum_i({\hat {y}}_{i}-{\overline {y}})^2$$


\end{itemize}

\textit{\small
Source with modifications: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{ [$R^2 = r_{X,Y}^2$] -- $ESS + SS_{res} = SS_{tot} $ (2/6)}}

\begin{itemize}
\item Now we focus on:
$$\sum_i2(y_{i}-{\hat {y}}_{i})({\hat {y}}_{i}-{\overline {y}}) = 2\sum_i(y_{i}-{\hat {y}}_{i})({\hat {y}}_{i}-{\overline {y}}) $$
\item and we want to prove that it is 0, that is $\sum_i(y_{i}-{\hat {y}}_{i})({\hat {y}}_{i}-{\overline {y}}) = 0$;  considering:
$$y_i = \hat{y_i} + \epsilon_i$$
$$ E(y_i) = E(\hat{y_i} + \epsilon_i) = E(\hat{y_i}) + E(\epsilon_i) = E(\hat{y_i}) $$
because $\epsilon$ is a stationary gaussian process $N(0,\sigma^2)$

\end{itemize}

\textit{\small
Source with modifications: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{ [$R^2 = r_{X,Y}^2$] -- $ESS + SS_{res} = SS_{tot} $ (3/6)}}

\begin{itemize}
\item We can build a system:
\[
\systeme*{
\hat{y}_i = \theta_0 + \theta_1 x_i,
\overline{y} = \theta_0 + \theta_1 \overline{x}
}
\]
\item from which we deduce by subtraction:
$$\hat{y}_i - \overline{y} = \theta_1 (x_i - \overline{x})$$
\item remembering that:
$$ \theta_1  = \frac { Cov(x,y)} { Var(x) } = {\frac {\sum _{i=1}^{n}(x_{i}-{\overline {x}})(y_{i}-{\overline {y}})}{\sum _{i=1}^{n}(x_{i}-{\overline {x}})^{2}}}$$
\end{itemize}

\textit{\small
Source with modifications: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}}
\end{frame}
% ============= =========== % ============= ===========


% ============= =========== % ============= ===========
\begin{frame}
{\centerline{ [$R^2 = r_{X,Y}^2$] -- $ESS + SS_{res} = SS_{tot} $ (4/6)}}

\begin{itemize}
\item So:
$$\sum_i(y_{i}-{\hat {y}}_{i})({\hat {y}}_{i}-{\overline {y}}) = \sum_i(y_{i}-{\hat {y}}_{i})(\theta_1 (x_i - \overline{x}))=$$
$$= \theta_1 \sum_i(y_{i}-{\hat {y}}_{i})(x_i - \overline{x})$$
\item Now, let's consider that:
$$(y_{i}-{\hat {y}}_{i})= y_{i}-{\hat {y}}_{i} + \overline {y} - \overline {y} = (y_{i}-{\overline {y}})-({\hat {y}}_{i}-{\overline {y}})= $$
$$ = (y_{i}-{\overline {y}})-{\theta_1}(x_{i}-{\overline {x}})$$
\item Substituting $(y_{i}-{\hat {y}}_{i})$ above we get:
$$ \theta_1 \sum_i(y_{i}-{\hat {y}}_{i})(x_i - \overline{x})=\theta_1 \sum_i[(y_{i}-{\overline {y}})-{\theta_1}(x_{i}-{\overline {x}}) ](x_i - \overline{x}) $$


\end{itemize}

\textit{\small
Source with modifications: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{ [$R^2 = r_{X,Y}^2$] -- $ESS + SS_{res} = SS_{tot} $ (5/6)}}

\begin{itemize}
\item We can conclude:
$$ \theta_1 \sum_i[(y_{i}-{\overline {y}})-{\theta_1}(x_{i}-{\overline {x}}) ](x_i - \overline{x}) = $$
$$ = \theta_1 [\sum_i (y_{i}-{\overline {y}}) (x_i - \overline{x}) -
\sum_i{\theta_1}(x_{i}-{\overline {x}}) (x_i - \overline{x})] = $$
$$ = \theta_1 [\sum_i (y_{i}-{\overline {y}}) (x_i - \overline{x}) -
\sum_i{{\frac {\sum _{j}(x_{j}-{\overline {x}})(y_{j}-{\overline {y}})}{\sum _{j}(x_{j}-{\overline {x}})^{2}}}
}(x_{i}-{\overline {x}})^2] = $$


\end{itemize}

\textit{\small
Source with modifications: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}}
\end{frame}
% ============= =========== % ============= ===========

\begin{frame}
{\centerline{ [$R^2 = r_{X,Y}^2$] -- $ESS + SS_{res} = SS_{tot} $ (6/6)}}

\begin{itemize}
\item And simplifying what is in [ $\bullet$ ]:

$$ \sum_i (y_{i}-{\overline {y}}) (x_i - \overline{x}) -
\sum_i{{\frac {\sum _{j}(x_{j}-{\overline {x}})(y_{j}-{\overline {y}})}{\sum _{j}(x_{j}-{\overline {x}})^{2}}}
}(x_{i}-{\overline {x}})^2 = $$
$$=  \sum_i (y_{i}-{\overline {y}}) (x_i - \overline{x}) -
\sum _{j}(x_{j}-{\overline {x}})(y_{j}-{\overline {y}})\sum_i{{\frac {(x_{i}-{\overline {x}})^2}{\sum _{j}(x_{j}-{\overline {x}})^{2}}}
}= $$
$$=  \sum_i (x_i - \overline{x}) (y_{i}-{\overline {y}})  -
\sum _{j}(x_{j}-{\overline {x}})(y_{j}-{\overline {y}}){\frac {\sum_i{(x_{i}-{\overline {x}})^2}}{\sum _{j}(x_{j}-{\overline {x}})^{2}}} = $$
$$=  \sum_i (x_i - \overline{x}) (y_{i}-{\overline {y}})  -
\sum _{j}(x_{j}-{\overline {x}})(y_{j}-{\overline {y}}) = 0 $$


QED.


\end{itemize}

\textit{\small
Source with modifications: \url{https://en.wikipedia.org/wiki/Explained_sum_of_squares}}
\end{frame}
% ============= =========== % ============= ===========


% ============= =========== % ============= ===========
\begin{frame}
{\centerline{$R^2 = r_{X,Y}^2$ (2/4)}}
\begin{itemize}
\item Now we know that, under the assumption to deal with a Gaussian noise centered in 0 we have:
$$ESS + SS_{res} = SS_{tot} $$
\item Under this hypothesis we have:
$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = \frac{SS_{tot}-SS_{res}}{SS_{tot}} = \frac{ESS}{SS_{tot}} $$
\end{itemize}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{$R^2 = r_{X,Y}^2$ (3/4)}}
\begin{itemize}
\item We now consider the square of $r_{X,Y} = r_{\hat{Y},Y}$
$$r_{\hat{Y},Y}^2= \left ( \frac{Cov(\hat{Y},Y)}{\sqrt{Var(Y)Var(\hat{Y})}} \right )^2 =  \frac{Cov(\hat{Y},Y)Cov(\hat{Y},Y)}{Var(Y)Var(\hat{Y})} = $$
$$ = \frac{Cov(\hat{Y},\hat{Y} + \epsilon)Cov(\hat{Y},\hat{Y} + \epsilon)}{Var(Y)Var(\hat{Y})} = $$
$$ 
= \frac{(Cov(\hat{Y},\hat{Y})+ Cov(\hat{Y},\epsilon))(Cov(\hat{Y},\hat{Y})+ Cov(\hat{Y},\epsilon))}{Var(Y)Var(\hat{Y})} = $$
$$ 
= \frac{Cov(\hat{Y},\hat{Y})Cov(\hat{Y},\hat{Y})}{Var(Y)Var(\hat{Y})} $$


\end{itemize}

\textit{\small
Source with modifications: \url{https://economictheoryblog.com/2014/11/05/proof/}}
\end{frame}
% ============= =========== % ============= ===========
% ============= =========== % ============= ===========
\begin{frame}
{\centerline{$R^2 = r_{X,Y}^2$ (4/4)}}
\begin{itemize}
\item But we know that $Cov(\hat{Y},\hat{Y}) = Var(\hat{Y})$, therefore we get that
$$r_{X,Y}^2 = \frac{Var(\hat{Y})Var(\hat{Y})}{Var(Y)Var(\hat{Y})} = \frac{Var(\hat{Y})}{Var(Y)}=$$
$$ = \frac{\frac{\sum_i{(\hat{y_i}-\overline{\hat{y}})^2}}{n}}{\frac{\sum_i(y_i-\overline{y})^2}{n}}
= \frac{{\sum_i{(\hat{y_i}-\overline{\hat{y}})^2}}}{\sum_i(y_i-\overline{y})^2} =  \frac{ESS}{SS_{tot}} $$
since we have already proven that $\overline{y} = \overline{\hat{y}}$
\end{itemize}
QED

\textit{\small
Source with modifications: \url{https://economictheoryblog.com/2014/11/05/proof/}}
\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========
\begin{frame}
{\centerline{Comment on $R^2 = r_{X,Y}^2$ }}
\begin{itemize}
\item This is a major result\\
\item It is the center of our subsequent investigation, in the case of normality of error we can model, interconnect, and understand relationships in an easy way\\
\item The next question is on how the slope of the regression line ($\theta_1$) relates to the correlation coefficient $r_{X,Y}$

\end{itemize}

\end{frame}
% ============= =========== % ============= ===========

\begin{frame}
{\centerline{$ r_{X,Y} $ and $\theta_1$}}
\begin{itemize}
\item We know that:
$$ \theta_1  = \frac { Cov(X,Y)} { Var(X) } $$
\item And that:
$$r_{X,Y} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$
\item Therefore:
$$ \theta_1  Var(X) = r_{X,Y}  \sigma_X\sigma_Y $$
\item We can then conclude that:
$$ \theta_1  = \frac{\sigma_X\sigma_Y}{Var(X)} r_{X,Y}   $$
$$ r_{X,Y}   = \frac{Var(X)}{\sigma_X\sigma_Y} \theta_1  $$

\end{itemize}

\end{frame}
% ============= =========== % ============= ===========

% ============= =========== % ============= ===========

\begin{frame}
{\centerline{Comment on $ r_{X,Y} ~ \sim ~ \theta_1$}}
\begin{itemize}
\item $ r_{X,Y} $ and $\theta_1$ are therefore directly and monotonically proportional
\item It means that a positive relationships implies a positive slope and viceversa
\end{itemize}

\end{frame}
% ============= =========== % ============= ===========


% https://en.wikipedia.org/wiki/Explained_sum_of_squares


\begin{frame}
{\centerline{General remark}}

\begin{itemize}
\item Right now we work with samples of larger populations of data
\item We measure properties of samples, like mean, standard deviation, covariance, correlation coefficient
\item All these properties are also random variable and have a distribution
\item Our question is therefore, what kind of distribution is the one of the correlation coefficient
\item Knowing its distribution allows us to understand the relationships existing between the variables it connect
\end{itemize}


\end{frame}



\begin{frame}
{\centerline{Non parametric correlations}}

\begin{itemize}
    \item Spearman's rank correlation coefficient
    \item Kendall's $\tau$
    \item $\ldots{}$

\end{itemize}

\end{frame}


\begin{frame}
{\centerline{Spearman's Rank Correlation Coeff. (1/3)}}

\begin{itemize}
   \item What can we do when the data is not normally distributed?
   \item Or even if the data is not on a ratio scale, just on an ordinal scale?\newline
   \item \textit{If the data is on a nominal scale, the concept of correlation looses interest; at most we can consider clustering.}
\end{itemize}


\end{frame}

\begin{frame}
{\centerline{Spearman's Rank Correlation Coeff. (2/3)}}
Idea:
\begin{itemize}
   \item Transform the data into ranks
   \item Apply the Pearson correlation coefficient to ranks
   \item Indeed, the values can be different, and also the significance and the mutual relationship
   \item Remember that:
   $$r_{X,Y} = \frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$
   \item And also that:
   $$ \theta_1  = \frac{\sigma_X\sigma_Y}{Var(X)} r_{X,Y}   $$
\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Spearman\%27s_rank_correlation_coefficient}}

\end{frame}

\begin{frame}
{\centerline{Spearman's Rank Correlation Coeff. (3/3)}}
Definition:
\begin{itemize}
   \item Let's have two sets $X = \{X_i\}$ and $Y= \{Y_i\}$ of the same size $n$ where $(\forall i) X_i,Y_i \in \text{ordinal scale}$
   \item Let's consider a set of pairs $P_{X,Y} = \{(X_i,Y_i)\}$ 
   \item Let's define 
   \begin{itemize}
   \item $(\forall X_i \in X) ~ rk{_X{_i}} = \text{rank}(X_i,X)$, $Rk_X = \{ rk_{X{_i}} \} $ 
   \item $(\forall Y_i \in Y) ~ rk_{Y{_i}} = \text{rank}(Y_i,Y)$, $Rk_Y = \{ rk_{Y{_i}} \} $ 
\end{itemize}
    \item We define the Spearman's Rank Correlation Coefficient between $X$ and $Y$, $r_S(X,Y)$ as:
    $$r_S(X,Y) = r(Rk_X, Rk_Y) = \frac{Cov(Rk_X,Rk_Y)}{\sigma_{Rk_X}\sigma_{Rk_Y}}$$
\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Spearman\%27s_rank_correlation_coefficient}}

\end{frame}

\begin{frame}
{\centerline{Visualization of $r_S$}}

Spearman's Rank Correlation Coefficient is based on monotonicity:

\begin{center}
\includegraphics[width=0.45\textwidth]{A2022.IDSEPC.Correlazione/600px-Spearman_fig4.png}
\includegraphics[width=0.45\textwidth]{A2022.IDSEPC.Correlazione/600px-Spearman_fig5.png}
\end{center} 

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Spearman\%27s_rank_correlation_coefficient}}

\end{frame}


\begin{frame}
{\centerline{$r$ and $r_S$ (1/2)}}

Indeed, the values of $r$ and $r_S$ can be different:

\begin{center}
\includegraphics[width=0.475\textwidth]{A2022.IDSEPC.Correlazione/600px-Spearman_fig1.png}
\includegraphics[width=0.475\textwidth]{A2022.IDSEPC.Correlazione/600px-Spearman_fig3.png}

\end{center} 

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Spearman\%27s_rank_correlation_coefficient}}

\end{frame}

\begin{frame}
{\centerline{$r$ and $r_S$ (2/2)}}

\begin{center}
\includegraphics[width=0.7\textwidth]{A2022.IDSEPC.Correlazione/Examples-of-Pearson-and-Spearman-correlations-In-each-subplot-r-is-Pearson-correlation.jpeg}
\end{center} 

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://www.researchgate.net/figure/Examples-of-Pearson-and-Spearman-correlations-In-each-subplot-r-is-Pearson-correlation_fig7_224915794}}

\end{frame}

\begin{frame}
{\centerline{About $r_S$}}
Note that:
\begin{itemize}
   \item Two identical values are assigned their fractional rank
   \begin{itemize}
   \item So if we have as values 20, 20, 30, 35, 36, then their ranks should be 1.5 (the average between 1 and 2), 1.5, 3, 4, 5 respectively
\end{itemize}
   \item Taking into account that we are dealing with integer ranks, we can simplify the formula as follows if all values are different:
   $$ r_{s}={1-{\frac {6\sum d_{i}^{2}}{n(n^{2}-1)}}}$$
   \begin{itemize}
   \item where each $d_i$ is equal to the difference in rank between $X_i$ and $Y_i$:
   $$d_i = Rk_{X_i} - Rk_{Y_i}$$
   \item and $n$ is the number of observations
\end{itemize}
\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Spearman\%27s_rank_correlation_coefficient}}

\end{frame}

\begin{frame}
{\centerline{Significance of $r_S$ (1/2)}}

\begin{itemize}
    \item Being based on ordinals and non assuming anything on the distribution of the underlying populations, the computation of the significance of $r_S$ is based on permutations
    \item This belong to the family of permutation tests
    \begin{itemize}
    \item A permutation test (or exact test) is a type of statistical significance test in which the distribution of the test statistic under the null hypothesis is obtained by calculating all possible values of the test statistic under rearrangements of the labels on the observed data points
    \end{itemize}
    \item In our case, since I have sequence of ordinals, we can consider all possible pairs of mutual relationships and, based on this, determine if the monotonic relationship that we have obtained is significantly different from a random order
    
\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Resampling_(statistics)\#Permutation_tests}}
\end{frame}

\begin{frame}
{\centerline{Significance of $r_S$ (2/2)}}

\begin{itemize}
    \item Consider as an example the dataset $\{(X_i,Y_i)\} = \{(10,2), (15, 0), (20, 4), (21,50)\}$
    \item Does it have a significant positive correlation?
    \item We need to assign ranks the elements, leading to $\{(Rk_{X_i},Rk_{Y_i})\} = \{(1,2), (2, 1), (3, 3), (4,4)\}$
    \item This leads to $r_S = 0.8$
    \item To compute the significance, I determine the number of times the comparison $Rk_{Y_i} \leq Rk_{Y_j}$ are true when $i<j$
    \item These are sequences of Bernoulli trials $\ldots$
    
\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Resampling_(statistics)\#Permutation_tests}}
\end{frame}


\begin{frame}
{\centerline{Kendall's $\tau$ (1/2)}}
An alternative non parametric correlation coefficient is the Kendall's $\tau$
\begin{itemize}
   \item Let's have two sets $X = \{X_i\}$ and $Y= \{Y_i\}$ of the same size $n$ where $(\forall i) X_i,Y_i \in \text{ordinal scale}$
   \item Let's consider a set of pairs $P_{X,Y} = \{(X_i,Y_i)\}$ 
   \item Let's assume that the two sets $X$ and $Y$ do not contain duplicates
   \item Let's define
   \begin{itemize}
    \item a concordant pair, a pair of pairs $(X_i,Y_i)$ and $(X_j,Y_j)$, with $i\neq j$ where either ($X_i > X_j$ and $Y_i > Y_j$) or ($X_i < X_j$ and $Y_i < Y_j$)
    \item a discordant pair, a pair of pairs $(X_i,Y_i)$ and $(X_j,Y_j)$, with $i\neq j$ where either ($X_i > X_j$ and $Y_i < Y_j$) or ($X_i > X_j$ and $Y_i < Y_j$)
\end{itemize}

\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient}}
\end{frame}

\begin{frame}
{\centerline{Kendall's $\tau$ (2/2)}}
\begin{itemize}
\item We can define the Kendall's $\tau$ as:
$$\tau ={\frac {({\text{\# concordant pairs}})-({\text{\# discordant pairs}})}{n(n-1)/2}}$$

\end{itemize}

\textit{\tiny
\vspace{-\baselineskip}
Source with modifications: \url{https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient}}
\end{frame}






\begin{frame}
{\centerline{Domande?}}
\vspace{1cm}
\begin{center}
    \LARGE{Fine della lezione dieci.}
\end{center}

\end{frame}


%----------------------------------------------------------------------------------------
\end{document}
